{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import nltk as nl\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import cmudict\n",
    "from math import sqrt,log\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = len(re.findall('[a-zA-Z0-9]',\"cartoons are okay but i need some actual entertainment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of everyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 15# sumthing you can set \n",
    "num_points =10489 # you can calculate\n",
    "Dataset = np.zeros([num_points, num_features])\n",
    "###we need a dictionary which can say which features is at which column \n",
    "feature_dict={}\n",
    "feature_dict['0'] = 'letters/wrd'\n",
    "feature_dict['1'] = 'syyl/wrd'\n",
    "feature_dict['2'] = 'wrd/sent'\n",
    "\n",
    "feature_dict['3'] = 'ttr'\n",
    "feature_dict['4'] = 'root_ttr'\n",
    "feature_dict['5'] = 'corrected_ttr'\n",
    "feature_dict['6'] = 'bilog_ttr'\n",
    "feature_dict['7'] = 'uber'\n",
    "\n",
    "feature_dict['8'] = 'noun_var'\n",
    "feature_dict['9'] = 'adverb_var'\n",
    "feature_dict['10'] = 'adjective_var'\n",
    "feature_dict['11'] = 'verb_var'\n",
    "feature_dict['12'] = 'sq_verb_var'\n",
    "feature_dict['13'] = 'cor_verb_var'\n",
    "feature_dict['14'] = 'lex_var'\n",
    "feature_dict['15'] = 'lex_dens'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finding number of syllabes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = cmudict.dict()\n",
    "\n",
    "def nsyl(word):\n",
    "    try:\n",
    "        x = [len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]]\n",
    "        return x[0]\n",
    "    except KeyError:\n",
    "        #print('error')\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)\n",
    "\n",
    "def syllables(word):\n",
    "#referred from stackoverflow.com/questions/14541303/count-the-number-of-syllables-in-a-word\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(word)):\n",
    "        if word[index] in vowels and word[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if word.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find all types of Type- token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ttr(file_content):\n",
    "    token = len(nl.word_tokenize(file_content))\n",
    "    ttype = len(set(nl.word_tokenize(file_content)))\n",
    "    ttr = ttype/token                             #Type Token ratio\n",
    "    rttr = ttype/sqrt(token)                      #root TTR\n",
    "    cttr = ttype/sqrt(2*token)                    # corrected TTR\n",
    "    bttr = log(ttype)/log(token)                  #Bilogarithmic TTR\n",
    "    if  (log(token/ttype))!=0:\n",
    "        uber = ((log(ttype)**2)/(log(token/ttype)) )     #Uber Index\n",
    "    else:\n",
    "        uber =0\n",
    "    return ttr,rttr,cttr,bttr,uber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nvar(file_contents):\n",
    "    ttokens = nl.word_tokenize(file_contents.lower())# total tokens \n",
    "    text = nl.Text(ttokens)        \n",
    "    tags = nl.pos_tag(text)\n",
    "    count = Counter(tag for word,tag in tags)\n",
    "    nn = count['NN']+count['NNS']+count['NNP']+count['NNPS'] ##only for noun\n",
    "    vb = count['VB']+count['VBD']+count['VBG']+count['VBN']+count['VBP']+count['VBZ'] # for verb\n",
    "    adv = count['RB']+count['RBR']+count['RBS']\n",
    "    adj = count['JJ']+count['JJR']+count['JJS']\n",
    "    lex_item = nn+vb+adv+adj\n",
    "    return nn,vb,adv,adj,count,lex_item\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loc = \"/home/sp/nlpfinal/WeeBit-TextOnly/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#just few declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readFeatures(fileData,nameList):\n",
    "    featuresDict = {}\n",
    "    index = 0\n",
    "    avgwps = {}\n",
    "    avglpw = {}\n",
    "    avgspw = {}\n",
    "    nv = {}\n",
    "    vv1 = {}\n",
    "    svv = {}\n",
    "    cvv = {}\n",
    "    advvv = {}\n",
    "    adjvv = {}\n",
    "    LD = {}\n",
    "    ttr = {}\n",
    "    rttr = {}\n",
    "    cttr = {}\n",
    "    bttr = {}\n",
    "    uber = {}\n",
    "    \n",
    "    for page in fileData:\n",
    "        \n",
    "        key = nameList[index]\n",
    "        index = index + 1\n",
    "        syl = 0\n",
    "        file_contents = page\n",
    "        words = nl.word_tokenize(file_contents)\n",
    "        dwords = len(words)\n",
    "        vocab = len(set(words))\n",
    "\n",
    "        letters = len(re.findall('[a-zA-Z0-9]',file_contents))\n",
    "        \n",
    "\n",
    "        dsent = len(nl.sent_tokenize(file_contents))\n",
    "        #avg number of word per sentence\n",
    "        avgwps[key] = dwords/dsent \n",
    "        #done\n",
    "        \n",
    "        #avg number of letter per word\n",
    "        avglpw[key] = letters/dwords\n",
    "\n",
    "\n",
    "       #avg number of syllables per word \n",
    "        for k in range(len(words)):\n",
    "            syl = syl+nsyl(words[k]) \n",
    "        avgspw[key] = syl/dwords \n",
    "\n",
    "        # will give number of noun,verb,adverb,adjective\n",
    "        nn,vb,adv,adj, v,lex_item = nvar(file_contents) \n",
    "\n",
    "        # noun veriations\n",
    "        nv[key] = nn/vocab  \n",
    "\n",
    "        #verb variations1\n",
    "        vv1[key] = vb /vocab   \n",
    "\n",
    "        #squared verb variations\n",
    "        svv[key] = (vb**2)/vocab \n",
    "\n",
    "         #corrected verb variations\n",
    "        cvv[key] = vb/sqrt(2*vocab) \n",
    "\n",
    "         #adverb variations\n",
    "        advvv[key] = adv/vocab    \n",
    "\n",
    "         #adjective variations\n",
    "        adjvv[key] = adj/vocab   \n",
    "\n",
    "         #lexical density\n",
    "        LD[key] = lex_item /dwords \n",
    "\n",
    "        #find ttr n its family \n",
    "        ttr[key],rttr[key],cttr[key],bttr[key],uber[key] = find_ttr(file_contents) \n",
    "        #print(i,uber[i-1])\n",
    "        \n",
    "    featuresDict[\"W/S\"] = avgwps\n",
    "    featuresDict[\"letter/W\"] = avglpw\n",
    "    featuresDict[\"Syl/W\"] = avgspw\n",
    "    featuresDict[\"NV\"] = nv\n",
    "    featuresDict[\"VV1\"] = vv1\n",
    "    featuresDict[\"svv\"] = svv\n",
    "    featuresDict[\"cvv\"] = cvv\n",
    "    featuresDict[\"advvv\"] = advvv\n",
    "    featuresDict[\"adjvv\"] = adjvv\n",
    "    featuresDict[\"LD\"] = LD\n",
    "    featuresDict[\"ttr\"] = ttr\n",
    "    featuresDict[\"rttr\"] = rttr\n",
    "    featuresDict[\"cttr\"] = cttr\n",
    "    featuresDict[\"bttr\"] = bttr\n",
    "    featuresDict[\"uber\"] = uber\n",
    "\n",
    "    return featuresDict\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataReader(flistName,location):\n",
    "    dataList = []\n",
    "    for fname in flistName:\n",
    "        file = open(location+fname,\"r\",encoding=\"cp437\")\n",
    "        data = file.read()\n",
    "        dataList.append(data)\n",
    "    return dataList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality code ends here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#Read the Data\n",
    "loc = \"/home/sp/nlpfinal/WeeBit-TextOnly/\"\n",
    "#BitGCSE, BitKS3, WRLevel2, WRLevel3, WRLevel4\n",
    "\n",
    "BitGCSE = loc+\"BitGCSE/\" # for age group 15 to 16 year\n",
    "BitKS3 = loc + \"BitKS3/\" # for age group 11 to 13 year\n",
    "WRLevel2 = loc + \"WRLevel2/\"\n",
    "WRLevel3 = loc + \"WRLevel3/\"\n",
    "WRLevel4 = loc + \"WRLevel4/\"\n",
    "l1 = os.listdir(WRLevel2)\n",
    "l2 = os.listdir(WRLevel3)\n",
    "l3 = os.listdir(WRLevel4)\n",
    "l4 = os.listdir(BitGCSE)\n",
    "l5 = os.listdir(BitKS3)\n",
    "\n",
    "l1Data = dataReader(l1,WRLevel2)\n",
    "l2Data = dataReader(l2,WRLevel3)\n",
    "l3Data = dataReader(l3,WRLevel4)\n",
    "l4Data = dataReader(l4,BitGCSE)\n",
    "l5Data = dataReader(l5,BitKS3)\n",
    "\n",
    "\n",
    "lexWR2 = readFeatures(l1Data,l1)\n",
    "print(\"1\")\n",
    "lexWR3 = readFeatures(l2Data,l2)\n",
    "print(\"2\")\n",
    "lexWR4 = readFeatures(l3Data,l3)\n",
    "print(\"3\")\n",
    "lexbitgcse = readFeatures(l4Data,l4)\n",
    "print(\"4\")\n",
    "lexbitks3 = readFeatures(l5Data,l5)\n",
    "print(\"5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saveObject(lexWR2,\"lexWR2\")\n",
    "# saveObject(lexWR3,\"lexWR3\")\n",
    "# saveObject(lexWR4,\"lexWR4\")\n",
    "# saveObject(lexbitks3,\"lexks3\")\n",
    "# saveObject(lexbitgcse,\"lexgcse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# How to use : create a folder name \"repo\" in the directory\n",
    "# where file is located\n",
    "# obj : the object required to save\n",
    "# name : the name for the host file of the data\n",
    "\n",
    "def saveObject(obj,name): \n",
    "    pickle.dump(obj,open( \"repo/\"+name+\".pkl\", \"wb\" ))\n",
    "\n",
    "def loadObject(name):\n",
    "    obj = pickle.load( open( \"repo/\"+name+\".pkl\", \"rb\" ) )\n",
    "    return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def dataReader(flistName,location):\n",
    "    dataList = []\n",
    "    for fname in flistName:\n",
    "        file = open(location+fname,\"r\",encoding=\"cp437\")\n",
    "        data = file.read()\n",
    "        dataList.append(data)\n",
    "    return dataList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W/S', 'letter/W', 'Syl/W', 'NV', 'VV1', 'svv', 'cvv', 'advvv', 'adjvv', 'LD', 'ttr', 'rttr', 'cttr', 'bttr', 'uber'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LD': {'12.txt': 0.5642857142857143,\n",
       "  '218.txt': 0.5371024734982333,\n",
       "  '370.txt': 0.5192307692307693,\n",
       "  '475.txt': 0.5502645502645502,\n",
       "  '536.txt': 0.4755244755244755},\n",
       " 'NV': {'12.txt': 0.4177215189873418,\n",
       "  '218.txt': 0.5,\n",
       "  '370.txt': 0.5238095238095238,\n",
       "  '475.txt': 0.47596153846153844,\n",
       "  '536.txt': 0.5367647058823529},\n",
       " 'Syl/W': {'12.txt': 1.35,\n",
       "  '218.txt': 1.2756183745583038,\n",
       "  '370.txt': 1.445054945054945,\n",
       "  '475.txt': 1.3280423280423281,\n",
       "  '536.txt': 1.2727272727272727},\n",
       " 'VV1': {'12.txt': 0.31645569620253167,\n",
       "  '218.txt': 0.26973684210526316,\n",
       "  '370.txt': 0.32275132275132273,\n",
       "  '475.txt': 0.27884615384615385,\n",
       "  '536.txt': 0.29411764705882354},\n",
       " 'W/S': {'12.txt': 11.666666666666666,\n",
       "  '218.txt': 9.129032258064516,\n",
       "  '370.txt': 13.481481481481481,\n",
       "  '475.txt': 11.117647058823529,\n",
       "  '536.txt': 9.533333333333333},\n",
       " 'adjvv': {'12.txt': 0.12658227848101267,\n",
       "  '218.txt': 0.19736842105263158,\n",
       "  '370.txt': 0.15343915343915343,\n",
       "  '475.txt': 0.1346153846153846,\n",
       "  '536.txt': 0.18382352941176472},\n",
       " 'advvv': {'12.txt': 0.10126582278481013,\n",
       "  '218.txt': 0.046052631578947366,\n",
       "  '370.txt': 0.037037037037037035,\n",
       "  '475.txt': 0.0673076923076923,\n",
       "  '536.txt': 0.04411764705882353},\n",
       " 'bttr': {'12.txt': 0.8842096369570686,\n",
       "  '218.txt': 0.8898995264561886,\n",
       "  '370.txt': 0.8888604795995005,\n",
       "  '475.txt': 0.8993484809958087,\n",
       "  '536.txt': 0.8685753180084518},\n",
       " 'cttr': {'12.txt': 4.721153006870855,\n",
       "  '218.txt': 6.389036545979819,\n",
       "  '370.txt': 7.004806042447406,\n",
       "  '475.txt': 7.5648868615144025,\n",
       "  '536.txt': 5.686445668048217},\n",
       " 'cvv': {'12.txt': 1.988893210439325,\n",
       "  '218.txt': 2.351511272173258,\n",
       "  '370.txt': 3.137501449229202,\n",
       "  '475.txt': 2.8436839595036685,\n",
       "  '536.txt': 2.42535625036333},\n",
       " 'letter/W': {'12.txt': 3.442857142857143,\n",
       "  '218.txt': 3.8303886925795054,\n",
       "  '370.txt': 3.8076923076923075,\n",
       "  '475.txt': 3.7804232804232805,\n",
       "  '536.txt': 3.6433566433566433},\n",
       " 'rttr': {'12.txt': 6.676718612355281,\n",
       "  '218.txt': 9.035462133822014,\n",
       "  '370.txt': 9.906291707022127,\n",
       "  '475.txt': 10.698365597371705,\n",
       "  '536.txt': 8.041848585451524},\n",
       " 'svv': {'12.txt': 7.9113924050632916,\n",
       "  '218.txt': 11.05921052631579,\n",
       "  '370.txt': 19.687830687830687,\n",
       "  '475.txt': 16.173076923076923,\n",
       "  '536.txt': 11.764705882352942},\n",
       " 'ttr': {'12.txt': 0.5642857142857143,\n",
       "  '218.txt': 0.5371024734982333,\n",
       "  '370.txt': 0.5192307692307693,\n",
       "  '475.txt': 0.5502645502645502,\n",
       "  '536.txt': 0.4755244755244755},\n",
       " 'uber': {'12.txt': 33.36640284908891,\n",
       "  '218.txt': 40.60608235889707,\n",
       "  '370.txt': 41.92191714482352,\n",
       "  '475.txt': 47.69234296440942,\n",
       "  '536.txt': 32.46734719067739}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
